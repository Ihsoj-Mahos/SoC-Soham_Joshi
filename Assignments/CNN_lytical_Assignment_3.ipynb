{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_lytical_Assignment_3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CNN-lytical Assignment-3\n",
        "<center>\n",
        "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcS7fZ0PJ4leQi4qtXR5Egv5YILqQqvzVSNtFg&usqp=CAU\">\n",
        "</center>\n",
        "\n",
        "*  In this assignment, we will use CNNs in [PyTorch](https://pytorch.org/docs/stable/index.html) for image classification.\n",
        "\n",
        "* We have been using MNIST by flattening 28$\\times$28 images to 784-sized vectors.\n",
        "\n",
        "* This time, we will classify images from the CIFAR-10 dataset - dimension is 32$\\times$32.\n",
        "\n",
        "* Much of this notebook remains the same as for Assignment 2, just minor changes would be needed and this assignment won't take long.\n",
        "\n",
        "**Feel free to redefine any function signatures below, just make sure the final cell remains the same.**"
      ],
      "metadata": {
        "id": "Xwo8D8V-uPsI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import libraries here\n",
        "PyTorch, NumPy, Matplotlib, ...\n",
        "Even when equipped with PyTorch, NumPy and Matplotlib make your work easier for visualization etc.\n",
        "\n",
        "Note the following method to **initialize the seed** for reproducibility of results, both for NumPy & PyTorch (CPU/CUDA)."
      ],
      "metadata": {
        "id": "QQNvtQCE_j1Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "CnnyxVTxqpZB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pickle\n",
        "import random\n",
        "import os\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    '''Sets the seed of the entire notebook so results are the same every time we run.\n",
        "    This is for REPRODUCIBILITY.'''\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    # # When running on the CuDNN backend, two further options must be set\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    # # Set a fixed value for the hash seed\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    \n",
        "set_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load *Dataset*\n",
        "Use the [pickle file](https://drive.google.com/file/d/1_IHICOfAsT7x63VBBuN1WMh4bDqtUThp/view?usp=sharing) shared for this assignment here."
      ],
      "metadata": {
        "id": "D6dAe4V0_3zC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "with open('/content/drive/MyDrive/train_cifar.pkl', 'rb') as f:\n",
        "  data = pickle.load(f)\n",
        "\n",
        "X = data['X']\n",
        "y = data['y']\n",
        "\n",
        "\n",
        "#split -- torch tensors formed\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size = 0.1)\n",
        "X_train, X_test, y_train, y_test = torch.tensor(X_train), torch.tensor(X_test), torch.tensor(y_train), torch.tensor(y_test)\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n"
      ],
      "metadata": {
        "id": "umr8-1EI_3ez",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "607b9855-c1f3-43ba-edcd-94ec45ba56de"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a Dataset Class\n",
        "In PyTorch, there is existing implementation of batch-splitting. You don't need to do it manually over here. Instead, just define a Dataset class and a Dataloader wrapping it.\n",
        "\n",
        "A dataset class must have 3 functions - ```__init__```, ```__len__```, ```__getitem__```. Their names are pretty self-explanatory. You can read more about this [here](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html).\n",
        "\n",
        "You will have to perform normalization, augmentation on the dataset here itself, have a look at [PyTorch Transforms](https://pytorch.org/vision/stable/transforms.html).\n",
        "\n",
        "**Note -** While initializing the dataset class object, make sure you only pass the numpy arrays for images and labels. So the ```__init__``` function should look like\n",
        "```\n",
        "    def __init__(self, X, y):\n",
        "```"
      ],
      "metadata": {
        "id": "ZjY5oNGRAK1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define your dataset class\n",
        "class CIFAR:\n",
        "    def __init__(self, trainset, transform=None, target_transform=None):\n",
        "        self.X = trainloader\n",
        "        self.y = y\n",
        "        '''\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "        '''\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.X[idx]\n",
        "        label = self.y[idx]\n",
        "        # returns the image as a row vector \n",
        "        '''\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        if self.target_transform:\n",
        "            label = self.target_transform(label)\n",
        "        '''\n",
        "        return image, label\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vifSrimqBGjH"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ```nn.Module``` for your model\n",
        "In this segment, define a class for your model, it has to inherit from the ```nn.Module``` class. You must define two functions here - ```__init__``` and ```forward```, again pretty self-explanatory. Helper functions can also be implemented, your choice!\n",
        "\n",
        "Look into the following ```torch``` layers beyond those you used in the second assignment and combine them to form your network, you can find more [here](https://pytorch.org/docs/stable/nn.html) -\n",
        "- [```nn.Conv2d```](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html)\n",
        "- [```nn.BatchNorm2d```](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html)\n"
      ],
      "metadata": {
        "id": "DOs6uifpBF8P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define a child class of nn.Module for your model\n",
        "# specify the architecture here itself\n",
        "# define a child class of nn.Module for your model\n",
        "# specify the architecture here itself\n",
        "\n",
        "\n",
        "# ADD CONV LAYERS HERE \n",
        "class Model(nn.Module) :\n",
        "  def __init__(self) :\n",
        "    super(Model, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "    self.pool = nn.MaxPool2d(2, 2)\n",
        "    self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "    self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "    self.fc2 = nn.Linear(120, 84)\n",
        "    self.fc3 = nn.Linear(84, 10)\n",
        "  def forward(self, x) : \n",
        "    x = self.pool(F.relu(self.conv1(x)))\n",
        "    x = self.pool(F.relu(self.conv2(x)))\n",
        "    x = torch.flatten(x, 1) \n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "6Mr6_5pzGRjp"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training loop\n",
        "You can write a training loop but usually writing it within a function helps so that you can train in multiple passes with just one function call if you still don't see convergence of the loss. ```display_step``` is for you to display results on the validation set (which you must not have trained upon).\n",
        "\n",
        "You will need to use ```zero_grad()```, ```backward()``` and multiple such functions here. Look for them in the tutorials given."
      ],
      "metadata": {
        "id": "tVTyirdELXlt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, optimizer, train_loader, display_step=None):\n",
        "  EPOCHS = 10\n",
        "  for epoch in range(EPOCHS):\n",
        "    for data in train_loader:\n",
        "        X, y = data\n",
        "\n",
        "        #to reshape the y to [64] from [64,1]\n",
        "        y = torch.reshape(y, (-1,))\n",
        "\n",
        "        #print(X.shape) : torch.Size([64, 1, 28, 28])\n",
        "        \n",
        "        #: torch.Size([64, 1, 28, 28])\n",
        "\n",
        "        #removing stored value of previous grad from buffer\n",
        "        model.zero_grad()\n",
        "\n",
        "        #forward-prop step\n",
        "        #print(X.view(-1, 28 * 28).shape) : torch.Size([64, 784])\n",
        "        output = model.forward(X.view(-1, 28 * 28).float())\n",
        "        #print(output.shape) : torch.Size([64, 10])\n",
        "        \n",
        "        #assert output.shape == y.shape\n",
        "        #negetive log likelihood loss\n",
        "        loss = F.nll_loss(output, y)\n",
        "        #loss = criterion(output, y)\n",
        "\n",
        "        #back-prop step\n",
        "        loss.backward()\n",
        "\n",
        "        #update step\n",
        "        optimizer.step()\n",
        "\n",
        "    #training set loss\n",
        "    print(\"Cost function after epoch \", epoch, \"is :\", loss.item())\n",
        "\n",
        "#optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "z0BnrNm8LN5s"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize weights\n",
        "Write a small function to initialize weights for your model. You don't need to do it individually for each layer, there are ways to do it in a simple ```for``` loop."
      ],
      "metadata": {
        "id": "g319ipPXMh0R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights(model):\n",
        "    if isinstance(model, nn.Linear):\n",
        "        torch.nn.init.xavier_uniform(model.weight)\n",
        "        model.bias.data.fill_(0.01)"
      ],
      "metadata": {
        "id": "GRqqKNLZMjDe"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction & Accuracy\n",
        "Prediction function should predict outputs using your trained model for a given **NumPy array** ```X_test``` and the output should be another **NumPy array**.\n",
        "\n",
        "The accuracy function would be the same as before."
      ],
      "metadata": {
        "id": "ivuHRGtfN3sE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(net, testloader):\n",
        "    classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "    correct_pred = {classname: 0 for classname in classes}\n",
        "    total_pred = {classname: 0 for classname in classes}\n",
        "\n",
        "    # again no gradients needed\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            outputs = net(images)\n",
        "            _, predictions = torch.max(outputs, 1)\n",
        "            # collect the correct predictions for each class\n",
        "            for label, prediction in zip(labels, predictions):\n",
        "                if label == prediction:\n",
        "                    correct_pred[classes[label]] += 1\n",
        "                total_pred[classes[label]] += 1\n",
        "\n",
        "\n",
        "    # print accuracy for each class\n",
        "    for classname, correct_count in correct_pred.items():\n",
        "        accuracy = 100 * float(correct_count) / total_pred[classname]\n",
        "        print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')"
      ],
      "metadata": {
        "id": "_nKROVpWOa6j"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Actually training your model\n",
        "- Create a model, initialize it. Define optimizer for the model as well as loss criterion (you can actually set the seed here again, just in case you did some ```rand``` calls above for testing your functions).\n",
        "- Define an instance of the dataset class, wrap it in a dataloader.\n",
        "- Call the train function and train your model!\n"
      ],
      "metadata": {
        "id": "8aA1EWZmMbQe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "model = Model()\n",
        "#init_weights(model)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "trainload = CIFAR(X_train, y_train)\n",
        "testload = CIFAR(X_test, y_test)\n",
        "trainloader = torch.utils.data.DataLoader(trainload, 1024, shuffle = True)\n",
        "testloader = torch.utils.data.DataLoader(testload, 1024, shuffle = True)\n",
        "train(model, optimizer, trainloader)"
      ],
      "metadata": {
        "id": "F8JG_XURNLmr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "outputId": "a327ea77-b543-46cf-df14-4cae3cf5603c"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-65-db1debbed0f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtrainloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mtestloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-62-564054a08eed>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, train_loader, display_step)\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mEPOCHS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-60-77f80cb6b53e>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# returns the image as a row vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'DataLoader' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run your model for the validation dataset\n",
        "Use your trained model to get predictions for the validation dataset you split earlier."
      ],
      "metadata": {
        "id": "OQsiK0-COe6E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy(model, testloader)"
      ],
      "metadata": {
        "id": "i_B_NUjUOq3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Submission\n",
        "To submit your solution, you will need to make a file with name ```model.py``` containing imports necessary to write the model class and the model class itself. It shouldn't do anything else when run. Also create a file ```dataset.py``` with the dataset class and all necessary imports. Other than this, save the trained model in a file named ```ass_2.pt```. When you are done with the assignment, commit the updated notebook, the ```model.py```, ```dataset.py``` class files and the ```ass_2.pt``` model-weights file to the repository."
      ],
      "metadata": {
        "id": "0f4W_facj-PA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "torch.save(model, 'ass_3.pt')\n",
        "files.download('ass_3.pt') # download the file from the Colab session for submission"
      ],
      "metadata": {
        "id": "7tknYAy1j92M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check if it got saved right!"
      ],
      "metadata": {
        "id": "flMRBW9Akhkg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the model, use predict function\n"
      ],
      "metadata": {
        "id": "-wA9nHzYkj1R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}